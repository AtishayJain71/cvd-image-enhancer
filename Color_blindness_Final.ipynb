{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdRjm_H5y4x6",
        "outputId": "d3d9433a-e7b0-4830-a8f8-a04b1e4d8b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.28.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-vqph_0wf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-vqph_0wf\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 536dc9d527074e3b15df5f6677ffe1f4e104a4ab\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.0.1)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.18.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (25.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (8.1.8)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (1.1.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (4.3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.8)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install gradio\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vfoEN1tSVqEW",
        "outputId": "e5ef4cf2-f9e8-4dfc-ba11-52c3c563b47b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://674bea1a87b34232b6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://674bea1a87b34232b6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_f10217.pkl: 178MB [00:01, 99.8MB/s]                          \n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 128273 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 154666 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 157490 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 14597 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 35661 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 37480 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 36614 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 35845 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 35158 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 34500 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 33690 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 32927 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 31823 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 11822 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 8494 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 1813 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 1813 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 190864 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 185710 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 183520 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 181226 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 179255 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 177337 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 175409 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 161031 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 181698 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 179258 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 176808 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 174461 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 170395 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 167269 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 583821 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 664595 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 650687 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 635300 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 619237 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 214679 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 223058 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 220516 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 218000 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 215820 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 166753 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 186921 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 183499 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 166753 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 186921 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 183499 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 430577 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 259569 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 243052 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 329046 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 409126 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 107109 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 393691 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 271065 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n",
            "<ipython-input-1-dfe4a5ab29b7>:223: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 394668 negative Z values that have been clipped to zero\n",
            "  adjusted_rgb = lab2rgb(adjusted_image)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://674bea1a87b34232b6.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "import gradio as gr\n",
        "\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# Dictionary of color blindness simulation matrices\n",
        "COLOR_BLINDNESS_MATRICES = {\n",
        "    \"protanopia\": np.array([[0.567, 0.433, 0.0], [0.558, 0.442, 0.0], [0.0, 0.242, 0.758]]),\n",
        "    \"deuteranopia\": np.array([[0.625, 0.375, 0.0], [0.7, 0.3, 0.0], [0.0, 0.3, 0.7]]),\n",
        "    \"tritanopia\": np.array([[0.95, 0.05, 0.0], [0.0, 0.433, 0.567], [0.0, 0.475, 0.525]]),\n",
        "    \"protanomaly\": np.array([[0.817, 0.183, 0.0], [0.333, 0.667, 0.0], [0.0, 0.125, 0.875]]),\n",
        "    \"deuteranomaly\": np.array([[0.8, 0.2, 0.0], [0.258, 0.742, 0.0], [0.0, 0.142, 0.858]]),\n",
        "    \"tritanomaly\": np.array([[0.967, 0.033, 0.0], [0.0, 0.733, 0.267], [0.0, 0.183, 0.817]]),\n",
        "    \"achromatopsia\": np.array([[0.299, 0.587, 0.114], [0.299, 0.587, 0.114], [0.299, 0.587, 0.114]]),\n",
        "    \"achromatomaly\": np.array([[0.618, 0.320, 0.062], [0.163, 0.775, 0.062], [0.163, 0.320, 0.516]])\n",
        "}\n",
        "\n",
        "# Create temporary directory for outputs\n",
        "os.makedirs(\"/tmp/cvd_outputs\", exist_ok=True)\n",
        "\n",
        "def simulate_color_blindness(image, deficiency_type):\n",
        "    \"\"\"Simulate color blindness by applying a transformation matrix.\"\"\"\n",
        "    if deficiency_type not in COLOR_BLINDNESS_MATRICES:\n",
        "        raise ValueError(f\"Invalid deficiency type. Choose from {list(COLOR_BLINDNESS_MATRICES.keys())}.\")\n",
        "\n",
        "    # If image is a file path, read it\n",
        "    if isinstance(image, str):\n",
        "        img = cv2.imread(image)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    else:\n",
        "        img = image.copy()\n",
        "        if len(img.shape) == 2:  # Grayscale\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "        elif img.shape[2] == 4:  # RGBA\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "    normalized_image = img.astype(float) / 255.0\n",
        "    transform_matrix = COLOR_BLINDNESS_MATRICES[deficiency_type]\n",
        "    transformed_image = np.dot(normalized_image.reshape(-1, 3), transform_matrix.T)\n",
        "    transformed_image = np.clip(transformed_image, 0, 1).reshape(img.shape)\n",
        "    transformed_image = (transformed_image * 255).astype(np.uint8)\n",
        "\n",
        "    return transformed_image\n",
        "\n",
        "def setup_detectron2_model():\n",
        "    \"\"\"Set up the Detectron2 configuration and predictor.\"\"\"\n",
        "    cfg = get_cfg()\n",
        "    cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "    return DefaultPredictor(cfg), cfg\n",
        "\n",
        "def detect_objects(image, predictor, cfg):\n",
        "    \"\"\"Detect objects using Detectron2 and return details.\"\"\"\n",
        "    # Convert to BGR for detectron2\n",
        "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    else:\n",
        "        image_bgr = image\n",
        "\n",
        "    outputs = predictor(image_bgr)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    boxes = instances.pred_boxes.tensor.numpy()\n",
        "    scores = instances.scores.numpy()\n",
        "    class_ids = instances.pred_classes.numpy()\n",
        "\n",
        "    # Get class names from metadata\n",
        "    metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "    class_names = metadata.thing_classes\n",
        "\n",
        "    # Visualize the detection output\n",
        "    v = Visualizer(image_bgr, metadata=metadata, scale=1.2)\n",
        "    vis_output = v.draw_instance_predictions(instances)\n",
        "    vis_image = vis_output.get_image()\n",
        "    vis_image = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display\n",
        "\n",
        "    # Format output\n",
        "    detected_objects = []\n",
        "    masks = instances.pred_masks.numpy() if instances.has(\"pred_masks\") else None\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        class_name = class_names[class_ids[i]] if class_ids[i] < len(class_names) else \"Unknown\"\n",
        "        bbox_data = {\n",
        "            \"class\": class_name,\n",
        "            \"confidence\": float(scores[i]),\n",
        "            \"bounding_box\": {\n",
        "                \"x1\": float(boxes[i][0]),\n",
        "                \"y1\": float(boxes[i][1]),\n",
        "                \"x2\": float(boxes[i][2]),\n",
        "                \"y2\": float(boxes[i][3]),\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if masks is not None:\n",
        "            mask = (masks[i] * 255).astype(np.uint8)\n",
        "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            bbox_data[\"edges\"] = [contour.reshape(-1, 2).tolist() for contour in contours]\n",
        "\n",
        "        detected_objects.append(bbox_data)\n",
        "\n",
        "    return detected_objects, vis_image\n",
        "\n",
        "def save_detected_objects_with_edges(detected_objects, output_path):\n",
        "    \"\"\"Save detected objects with edges to a JSON file.\"\"\"\n",
        "    edge_data = [obj for obj in detected_objects if \"edges\" in obj]\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(edge_data, f, indent=2)\n",
        "    return output_path\n",
        "\n",
        "def analyze_iou_and_lab_similarity(image_rgb, json_path, color_threshold=20.0, iou_threshold=0):\n",
        "    \"\"\"Analyze detected objects based on IoU and LAB color similarity.\"\"\"\n",
        "    height, width = image_rgb.shape[:2]\n",
        "\n",
        "    with open(json_path, \"r\") as f:\n",
        "        objects = json.load(f)\n",
        "\n",
        "    # Convert contour edges to mask\n",
        "    def contour_to_mask(edges):\n",
        "        mask = np.zeros((height, width), dtype=np.uint8)\n",
        "        for edge in edges:\n",
        "            contour = np.array(edge, dtype=np.int32).reshape((-1, 1, 2))\n",
        "            cv2.drawContours(mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
        "        return mask\n",
        "\n",
        "    # Get average LAB color for the mask\n",
        "    def get_avg_lab(mask, image_rgb):\n",
        "        masked_pixels = image_rgb[mask == 255]\n",
        "        if masked_pixels.size == 0:\n",
        "            return np.zeros(3)\n",
        "        lab = rgb2lab(masked_pixels.reshape(-1, 1, 3) / 255.0).reshape(-1, 3)\n",
        "        return np.mean(lab, axis=0)\n",
        "\n",
        "    # Compute Intersection over Union (IoU)\n",
        "    def compute_iou(mask1, mask2):\n",
        "        inter = np.logical_and(mask1 == 255, mask2 == 255).sum()\n",
        "        union = np.logical_or(mask1 == 255, mask2 == 255).sum()\n",
        "        return inter / union if union > 0 else 0\n",
        "\n",
        "    # Prepare masks and LAB values for each object\n",
        "    masks = [contour_to_mask(obj[\"edges\"]) for obj in objects]\n",
        "    lab_means = [get_avg_lab(mask, image_rgb) for mask in masks]\n",
        "\n",
        "    # Compare all pairs based on IoU and LAB color similarity\n",
        "    similar_pairs = []\n",
        "    for i in range(len(objects)):\n",
        "        for j in range(i + 1, len(objects)):\n",
        "            iou = compute_iou(masks[i], masks[j])\n",
        "            if iou >= iou_threshold:\n",
        "                deltaE = np.linalg.norm(lab_means[i] - lab_means[j])\n",
        "                if deltaE < color_threshold:\n",
        "                    similar_pairs.append({\n",
        "                        \"obj1_index\": i,\n",
        "                        \"obj2_index\": j,\n",
        "                        \"label1\": objects[i][\"class\"],\n",
        "                        \"label2\": objects[j][\"class\"],\n",
        "                        \"iou\": float(iou),\n",
        "                        \"deltaE\": float(deltaE)\n",
        "                    })\n",
        "\n",
        "    # Highlight edges of similar objects\n",
        "    highlighted = image_rgb.copy()\n",
        "    for pair in similar_pairs:\n",
        "        for obj_id in [pair[\"obj1_index\"], pair[\"obj2_index\"]]:\n",
        "            contours = [np.array(edge, np.int32).reshape((-1, 1, 2)) for edge in objects[obj_id][\"edges\"]]\n",
        "            cv2.drawContours(highlighted, contours, -1, (0, 255, 0), thickness=3)\n",
        "\n",
        "    return similar_pairs, highlighted, masks\n",
        "\n",
        "def adjust_similar_object_colors(image_rgb, masks, similar_pairs, color_shift=60, shift_mode='auto'):\n",
        "    \"\"\"Shifts the color of one object in each similar-colored object pair.\"\"\"\n",
        "    # Convert to LAB for better color manipulation\n",
        "    lab_image = rgb2lab(image_rgb.astype(np.float32) / 255.0)\n",
        "    adjusted_image = lab_image.copy()\n",
        "    adjusted_ids = set()\n",
        "\n",
        "    for pair in similar_pairs:\n",
        "        target_id = pair[\"obj2_index\"]\n",
        "        if target_id in adjusted_ids:\n",
        "            continue  # Skip if already adjusted\n",
        "\n",
        "        mask = masks[target_id]\n",
        "        object_pixels = adjusted_image[mask == 255]\n",
        "\n",
        "        if object_pixels.size == 0:\n",
        "            continue  # Skip empty masks\n",
        "\n",
        "        # Compute mean LAB color\n",
        "        mean_color = np.mean(object_pixels, axis=0)\n",
        "\n",
        "        # Determine shift direction\n",
        "        if shift_mode == 'a':\n",
        "            shift = np.array([0, color_shift, 0])\n",
        "        elif shift_mode == 'b':\n",
        "            shift = np.array([0, 0, color_shift])\n",
        "        else:  # auto\n",
        "            shift = np.array([0, 0, 0])\n",
        "            if abs(mean_color[1]) < abs(mean_color[2]):\n",
        "                shift[1] = color_shift  # a channel\n",
        "            else:\n",
        "                shift[2] = color_shift  # b channel\n",
        "\n",
        "        # Apply shift\n",
        "        for i in range(3):\n",
        "            channel = adjusted_image[:, :, i]\n",
        "            channel[mask == 255] = np.clip(channel[mask == 255] + shift[i], -128 if i > 0 else 0, 127 if i > 0 else 100)\n",
        "            adjusted_image[:, :, i] = channel\n",
        "\n",
        "        adjusted_ids.add(target_id)\n",
        "\n",
        "    # Convert back to RGB\n",
        "    adjusted_rgb = lab2rgb(adjusted_image)\n",
        "    adjusted_rgb = (adjusted_rgb * 255).astype(np.uint8)\n",
        "\n",
        "    return adjusted_rgb\n",
        "\n",
        "def simulate_color_blindness_on_region(rgb_image, deficiency_type, region_mask):\n",
        "    \"\"\"Applies color blindness simulation only on a masked region of the image.\"\"\"\n",
        "    if deficiency_type not in COLOR_BLINDNESS_MATRICES:\n",
        "        raise ValueError(f\"Invalid deficiency type. Choose from {list(COLOR_BLINDNESS_MATRICES.keys())}.\")\n",
        "\n",
        "    transformed_image = rgb_image.copy().astype(np.float32) / 255.0\n",
        "    mask_indices = np.where(region_mask == 255)\n",
        "\n",
        "    # Extract only the masked pixels\n",
        "    pixels = transformed_image[mask_indices]\n",
        "    transformed_pixels = np.dot(pixels, COLOR_BLINDNESS_MATRICES[deficiency_type].T)\n",
        "    transformed_pixels = np.clip(transformed_pixels, 0, 1)\n",
        "\n",
        "    # Update only masked region in the image\n",
        "    transformed_image[mask_indices] = transformed_pixels\n",
        "    transformed_image = (transformed_image * 255).astype(np.uint8)\n",
        "\n",
        "    return transformed_image\n",
        "\n",
        "def get_default_shift_mode(deficiency_type):\n",
        "    \"\"\"Determine the optimal shift mode based on CVD type\"\"\"\n",
        "    if deficiency_type in ['protanopia', 'deuteranopia', 'protanomaly', 'deuteranomaly']:\n",
        "        # Red-green color blindness - shift on blue-yellow (b) axis\n",
        "        return 'b'\n",
        "    elif deficiency_type in ['tritanopia', 'tritanomaly']:\n",
        "        # Blue-yellow color blindness - shift on red-green (a) axis\n",
        "        return 'a'\n",
        "    else:\n",
        "        # For achromatopsia or achromatomaly, try both\n",
        "        return 'auto'\n",
        "\n",
        "def run_cvd_enhancement(image, deficiency_type, color_threshold, iou_threshold, color_shift, shift_mode, max_iterations):\n",
        "    \"\"\"Full pipeline for CVD enhancement\"\"\"\n",
        "    # Initialize outputs\n",
        "    results = {}\n",
        "\n",
        "    # Convert uploaded image to RGB\n",
        "    if image is None:\n",
        "        return None, None, None, None, None, \"Error: No image uploaded\"\n",
        "\n",
        "    if isinstance(image, str):\n",
        "        image = cv2.imread(image)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    elif len(image.shape) == 2:  # Grayscale\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    elif image.shape[2] == 4:  # RGBA\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "    # Save original image\n",
        "    original_path = \"/tmp/cvd_outputs/original.jpg\"\n",
        "    cv2.imwrite(original_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "    results[\"original\"] = image\n",
        "\n",
        "    # Step 1: Simulate color blindness\n",
        "    transformed_image = simulate_color_blindness(image, deficiency_type)\n",
        "    results[\"cvd_simulated\"] = transformed_image\n",
        "\n",
        "    # Step 2: Setup model and detect objects\n",
        "    predictor, cfg = setup_detectron2_model()\n",
        "    detected_objects, vis_image = detect_objects(image, predictor, cfg)\n",
        "    results[\"detection\"] = vis_image\n",
        "\n",
        "    # Step 3: Save detected objects\n",
        "    json_path = \"/tmp/cvd_outputs/detected_objects_with_edges.json\"\n",
        "    save_detected_objects_with_edges(detected_objects, json_path)\n",
        "\n",
        "    # Step 4: Analyze similarity\n",
        "    similar_pairs, highlighted, masks = analyze_iou_and_lab_similarity(\n",
        "        transformed_image,\n",
        "        json_path,\n",
        "        color_threshold=float(color_threshold),\n",
        "        iou_threshold=float(iou_threshold)\n",
        "    )\n",
        "    results[\"similar_objects\"] = highlighted\n",
        "\n",
        "    if not similar_pairs:\n",
        "        return (results.get(\"original\"),\n",
        "                results.get(\"cvd_simulated\"),\n",
        "                results.get(\"detection\"),\n",
        "                results.get(\"similar_objects\"),\n",
        "                results.get(\"enhanced_image\", transformed_image),\n",
        "                f\"No similar object pairs found. No enhancement needed.\")\n",
        "\n",
        "    # Step 5: Initialize for iterative enhancement\n",
        "    iteration = 0\n",
        "    adjusted_image = transformed_image.copy()\n",
        "\n",
        "    log_messages = [f\"Found {len(similar_pairs)} similar object pairs. Starting enhancement...\"]\n",
        "\n",
        "    # Step 6: Iterative enhancement\n",
        "    while iteration < int(max_iterations) and similar_pairs:\n",
        "        # Enhance colors\n",
        "        adjusted_rgb = adjust_similar_object_colors(\n",
        "            adjusted_image,\n",
        "            masks,\n",
        "            similar_pairs,\n",
        "            color_shift=float(color_shift),\n",
        "            shift_mode=shift_mode\n",
        "        )\n",
        "\n",
        "        # Create combined mask for all adjusted objects\n",
        "        adjusted_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "        for pair in similar_pairs:\n",
        "            adjusted_mask = cv2.bitwise_or(adjusted_mask, masks[pair[\"obj2_index\"]])\n",
        "\n",
        "        # Apply color blindness simulation to the adjusted regions\n",
        "        adjusted_image = simulate_color_blindness_on_region(adjusted_rgb, deficiency_type, adjusted_mask)\n",
        "\n",
        "        # Re-analyze similarity after adjustment\n",
        "        similar_pairs, highlighted, masks = analyze_iou_and_lab_similarity(\n",
        "            adjusted_image,\n",
        "            json_path,\n",
        "            color_threshold=float(color_threshold),\n",
        "            iou_threshold=float(iou_threshold)\n",
        "        )\n",
        "\n",
        "        log_messages.append(f\"Iteration {iteration + 1}: {len(similar_pairs)} similar pairs remaining\")\n",
        "\n",
        "        if len(similar_pairs) == 0:\n",
        "            log_messages.append(f\"All similar objects now distinguishable. Enhancement complete!\")\n",
        "            break\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    results[\"enhanced_image\"] = adjusted_image\n",
        "\n",
        "    # Return results\n",
        "    return (results.get(\"original\"),\n",
        "            results.get(\"cvd_simulated\"),\n",
        "            results.get(\"detection\"),\n",
        "            results.get(\"similar_objects\"),\n",
        "            results.get(\"enhanced_image\"),\n",
        "            \"\\n\".join(log_messages))\n",
        "\n",
        "# Create the Gradio interface\n",
        "def create_interface():\n",
        "    # Custom CSS for better styling\n",
        "    custom_css = \"\"\"\n",
        "    .app-header {\n",
        "        text-align: center;\n",
        "        margin: 0 auto;\n",
        "        max-width: 1200px;\n",
        "        padding: 1rem 0;\n",
        "    }\n",
        "\n",
        "    .app-title {\n",
        "        font-size: 2.5rem !important;\n",
        "        font-weight: bold;\n",
        "        margin-bottom: 0.5rem;\n",
        "        color: #1f77b4;\n",
        "    }\n",
        "\n",
        "    .app-description {\n",
        "        font-size: 1.2rem !important;\n",
        "        margin-bottom: 2rem;\n",
        "        max-width: 900px;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "    }\n",
        "\n",
        "    .how-it-works {\n",
        "        background-color: #222222;\n",
        "        color: #ffffff;\n",
        "        border-radius: 8px;\n",
        "        padding: 1.5rem;\n",
        "        margin-bottom: 2rem;\n",
        "        font-size: 1.2rem !important;\n",
        "        max-width: 1000px;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "        box-shadow: 0 2px 10px rgba(0,0,0,0.2);\n",
        "    }\n",
        "\n",
        "    .how-it-works h3 {\n",
        "        text-align: center;\n",
        "        font-size: 1.5rem !important;\n",
        "        margin-bottom: 1rem;\n",
        "        color: #ffffff;\n",
        "    }\n",
        "\n",
        "    .how-it-works ol {\n",
        "        padding-left: 2rem;\n",
        "    }\n",
        "\n",
        "    .how-it-works li {\n",
        "        margin-bottom: 0.5rem;\n",
        "    }\n",
        "\n",
        "    .how-it-works p {\n",
        "        margin-top: 1rem;\n",
        "    }\n",
        "\n",
        "    /* Fix image scaling */\n",
        "    .upload-image-container .wrap {\n",
        "        display: flex !important;\n",
        "        justify-content: center !important;\n",
        "        align-items: center !important;\n",
        "        height: auto !important;\n",
        "        min-height: 450px !important;\n",
        "    }\n",
        "\n",
        "    .upload-image-container img {\n",
        "        max-width: 100% !important;\n",
        "        max-height: 100% !important;\n",
        "        width: auto !important;\n",
        "        height: auto !important;\n",
        "        object-fit: contain !important;\n",
        "    }\n",
        "\n",
        "    .image-results-container {\n",
        "        flex-wrap: wrap !important;\n",
        "        justify-content: center !important;\n",
        "    }\n",
        "\n",
        "    /* Result images */\n",
        "    .result-image-container {\n",
        "        height: auto !important;\n",
        "        min-height: 450px !important;\n",
        "        display: flex !important;\n",
        "        align-items: center !important;\n",
        "        justify-content: center !important;\n",
        "    }\n",
        "\n",
        "    .result-image-container img {\n",
        "        max-width: 100% !important;\n",
        "        max-height: 100% !important;\n",
        "        width: auto !important;\n",
        "        height: auto !important;\n",
        "        object-fit: contain !important;\n",
        "        margin: 0 auto !important;\n",
        "    }\n",
        "\n",
        "    .results-header {\n",
        "        text-align: center;\n",
        "        font-size: 2rem !important;\n",
        "        margin: 1.5rem 0;\n",
        "    }\n",
        "\n",
        "    .result-column h3 {\n",
        "        text-align: center;\n",
        "        font-size: 1.5rem !important;\n",
        "        margin-bottom: 0.5rem;\n",
        "    }\n",
        "\n",
        "    .footer-button {\n",
        "        display: block;\n",
        "        margin: 2rem auto;\n",
        "        max-width: 300px;\n",
        "    }\n",
        "\n",
        "    /* Tab styling */\n",
        "    .detail-tabs .tab-nav {\n",
        "        background-color: #f5f5f5;\n",
        "        border-radius: 6px;\n",
        "        padding: 5px;\n",
        "    }\n",
        "\n",
        "    .detail-tabs .tabitem {\n",
        "        padding: 0.5rem !important;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"Color Vision Deficiency Enhancement Tool\", css=custom_css) as app:\n",
        "        # Title and description section (always visible)\n",
        "        with gr.Row(elem_classes=[\"app-header\"]):\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"# Color Vision Deficiency Enhancement Tool\", elem_classes=[\"app-title\"])\n",
        "                gr.Markdown(\n",
        "                    \"Transform images for better visibility by people with color vision deficiency. \"\n",
        "                    \"This intelligent system detects objects in images, identifies those that might appear similar to people with color blindness, \"\n",
        "                    \"and enhances color differences to make them more distinguishable.\",\n",
        "                    elem_classes=[\"app-description\"]\n",
        "                )\n",
        "\n",
        "        # How It Works section - centralized above the input section\n",
        "        with gr.Row(visible=True, elem_classes=[\"how-it-works-container\"]) as how_it_works_section:\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"\"\"\n",
        "                <div class=\"how-it-works\">\n",
        "                <h3>How It Works</h3>\n",
        "\n",
        "                <ol>\n",
        "                <li><strong>Upload</strong> your image that needs enhancement</li>\n",
        "                <li><strong>Select</strong> the type of color vision deficiency to accommodate</li>\n",
        "                <li>The system <strong>detects objects</strong> in your image using AI</li>\n",
        "                <li>It <strong>finds objects</strong> that might look similar to someone with the selected CVD</li>\n",
        "                <li>Colors are <strong>enhanced</strong> to make objects more distinguishable</li>\n",
        "                <li>View side-by-side <strong>comparisons</strong> of original, CVD simulation, and enhanced versions</li>\n",
        "                </ol>\n",
        "\n",
        "                <p>Fine-tune the enhancement with advanced settings for optimal results.</p>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "        # Input section\n",
        "        with gr.Row(visible=True) as input_section:\n",
        "            # Left panel - Image upload\n",
        "            with gr.Column(scale=3):\n",
        "                input_image = gr.Image(\n",
        "                    label=\"Upload Image\",\n",
        "                    type=\"numpy\",\n",
        "                    elem_classes=[\"upload-image-container\"],\n",
        "                    height=450,\n",
        "                    image_mode=\"RGB\",\n",
        "                    sources=[\"upload\", \"clipboard\"]\n",
        "                )\n",
        "\n",
        "            # Right panel - Settings\n",
        "            with gr.Column(scale=2):\n",
        "                deficiency_type = gr.Dropdown(\n",
        "                    choices=list(COLOR_BLINDNESS_MATRICES.keys()),\n",
        "                    value=\"protanopia\",\n",
        "                    label=\"Color Vision Deficiency Type\"\n",
        "                )\n",
        "\n",
        "                with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "                    color_threshold = gr.Slider(minimum=5, maximum=50, value=20, step=1,\n",
        "                                              label=\"Color Similarity Threshold (E)\")\n",
        "                    iou_threshold = gr.Slider(minimum=0, maximum=0.5, value=0, step=0.01,\n",
        "                                            label=\"Object Overlap Threshold (IoU)\")\n",
        "                    color_shift = gr.Slider(minimum=20, maximum=100, value=50, step=5,\n",
        "                                          label=\"Color Shift Amount\")\n",
        "                    shift_mode = gr.Radio([\"auto\", \"a\", \"b\"], value=\"b\",\n",
        "                                        label=\"Color Shift Mode\")\n",
        "                    max_iterations = gr.Slider(minimum=1, maximum=10, value=3, step=1,\n",
        "                                             label=\"Maximum Enhancement Iterations\")\n",
        "\n",
        "                # Update shift mode based on deficiency type\n",
        "                deficiency_type.change(\n",
        "                    fn=lambda x: gr.update(value=get_default_shift_mode(x)),\n",
        "                    inputs=[deficiency_type],\n",
        "                    outputs=[shift_mode]\n",
        "                )\n",
        "\n",
        "                run_button = gr.Button(\"Run Enhancement\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        # Processing log (temporarily visible during processing)\n",
        "        with gr.Row(visible=False) as processing_section:\n",
        "            output_log = gr.Textbox(label=\"Processing Status\", lines=3)\n",
        "\n",
        "        # Results header\n",
        "        with gr.Row(visible=False, elem_classes=[\"results-header\"]) as results_header:\n",
        "            gr.Markdown(\"## Results\")\n",
        "\n",
        "        # Main results row - showing 3 main images\n",
        "        with gr.Row(visible=False, equal_height=True) as main_results:\n",
        "            with gr.Column(elem_classes=[\"result-column\"]):\n",
        "                gr.Markdown(\"### Original\")\n",
        "                original_output = gr.Image(elem_classes=[\"result-image-container\"])\n",
        "\n",
        "            with gr.Column(elem_classes=[\"result-column\"]):\n",
        "                gr.Markdown(\"### CVD Simulation\")\n",
        "                cvd_output = gr.Image(elem_classes=[\"result-image-container\"])\n",
        "\n",
        "            with gr.Column(elem_classes=[\"result-column\"]):\n",
        "                gr.Markdown(\"### Enhanced for CVD\")\n",
        "                enhanced_output = gr.Image(elem_classes=[\"result-image-container\"])\n",
        "\n",
        "        # Optional detailed view with tabs (same scale as main results)\n",
        "        with gr.Row(visible=False) as detailed_results:\n",
        "            # Use a Column to make it span the full width\n",
        "            with gr.Column():\n",
        "                with gr.Tabs(elem_classes=[\"detail-tabs\"]) as tabs:\n",
        "                    # FIX: Remove the 'selected=True' parameter from TabItem\n",
        "                    with gr.TabItem(\"Object Detection\", id=\"tab-detection\"):\n",
        "                        detection_output = gr.Image(\n",
        "                            elem_classes=[\"result-image-container\"],\n",
        "                            height=350\n",
        "                        )\n",
        "\n",
        "                    with gr.TabItem(\"Similar Objects\", id=\"tab-similar\"):\n",
        "                        similar_output = gr.Image(\n",
        "                            elem_classes=[\"result-image-container\"],\n",
        "                            height=350\n",
        "                        )\n",
        "\n",
        "        # Rerun button (visible after processing)\n",
        "        with gr.Row(visible=False) as rerun_section:\n",
        "            rerun_button = gr.Button(\"Process Another Image\", variant=\"secondary\", size=\"lg\", elem_classes=[\"footer-button\"])\n",
        "\n",
        "        # Define workflow\n",
        "        def show_processing():\n",
        "            return {\n",
        "                input_section: gr.update(visible=False),\n",
        "                how_it_works_section: gr.update(visible=False),\n",
        "                processing_section: gr.update(visible=True),\n",
        "                results_header: gr.update(visible=False),\n",
        "                main_results: gr.update(visible=False),\n",
        "                detailed_results: gr.update(visible=False),\n",
        "                rerun_section: gr.update(visible=False)\n",
        "            }\n",
        "\n",
        "        def show_results():\n",
        "            return {\n",
        "                input_section: gr.update(visible=False),\n",
        "                how_it_works_section: gr.update(visible=False),\n",
        "                processing_section: gr.update(visible=False),\n",
        "                results_header: gr.update(visible=True),\n",
        "                main_results: gr.update(visible=True),\n",
        "                detailed_results: gr.update(visible=True),\n",
        "                rerun_section: gr.update(visible=True)\n",
        "            }\n",
        "\n",
        "        def reset_interface():\n",
        "            return {\n",
        "                input_section: gr.update(visible=True),\n",
        "                how_it_works_section: gr.update(visible=True),\n",
        "                processing_section: gr.update(visible=False),\n",
        "                results_header: gr.update(visible=False),\n",
        "                main_results: gr.update(visible=False),\n",
        "                detailed_results: gr.update(visible=False),\n",
        "                rerun_section: gr.update(visible=False)\n",
        "            }\n",
        "\n",
        "        # Connect event handlers\n",
        "        run_button.click(\n",
        "            fn=show_processing,\n",
        "            outputs=[input_section, how_it_works_section, processing_section, results_header, main_results, detailed_results, rerun_section]\n",
        "        ).then(\n",
        "            fn=run_cvd_enhancement,\n",
        "            inputs=[input_image, deficiency_type, color_threshold, iou_threshold, color_shift, shift_mode, max_iterations],\n",
        "            outputs=[original_output, cvd_output, detection_output, similar_output, enhanced_output, output_log]\n",
        "        ).then(\n",
        "            fn=show_results,\n",
        "            outputs=[input_section, how_it_works_section, processing_section, results_header, main_results, detailed_results, rerun_section]\n",
        "        )\n",
        "\n",
        "        rerun_button.click(\n",
        "            fn=reset_interface,\n",
        "            outputs=[input_section, how_it_works_section, processing_section, results_header, main_results, detailed_results, rerun_section]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Load and run the interface\n",
        "demo = create_interface()\n",
        "demo.launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
