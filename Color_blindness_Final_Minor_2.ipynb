{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdRjm_H5y4x6",
        "outputId": "d3d9433a-e7b0-4830-a8f8-a04b1e4d8b06"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install gradio\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vfoEN1tSVqEW",
        "outputId": "e5ef4cf2-f9e8-4dfc-ba11-52c3c563b47b"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "import gradio as gr\n",
        "\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# Dictionary of color blindness simulation matrices\n",
        "COLOR_BLINDNESS_MATRICES = {\n",
        "    \"protanopia\": np.array([[0.567, 0.433, 0.0], [0.558, 0.442, 0.0], [0.0, 0.242, 0.758]]),\n",
        "    \"deuteranopia\": np.array([[0.625, 0.375, 0.0], [0.7, 0.3, 0.0], [0.0, 0.3, 0.7]]),\n",
        "    \"tritanopia\": np.array([[0.95, 0.05, 0.0], [0.0, 0.433, 0.567], [0.0, 0.475, 0.525]]),\n",
        "    \"protanomaly\": np.array([[0.817, 0.183, 0.0], [0.333, 0.667, 0.0], [0.0, 0.125, 0.875]]),\n",
        "    \"deuteranomaly\": np.array([[0.8, 0.2, 0.0], [0.258, 0.742, 0.0], [0.0, 0.142, 0.858]]),\n",
        "    \"tritanomaly\": np.array([[0.967, 0.033, 0.0], [0.0, 0.733, 0.267], [0.0, 0.183, 0.817]]),\n",
        "    \"achromatopsia\": np.array([[0.299, 0.587, 0.114], [0.299, 0.587, 0.114], [0.299, 0.587, 0.114]]),\n",
        "    \"achromatomaly\": np.array([[0.618, 0.320, 0.062], [0.163, 0.775, 0.062], [0.163, 0.320, 0.516]])\n",
        "}\n",
        "\n",
        "# Create temporary directory for outputs\n",
        "os.makedirs(\"/tmp/cvd_outputs\", exist_ok=True)\n",
        "\n",
        "def simulate_color_blindness(image, deficiency_type):\n",
        "    \"\"\"Simulate color blindness by applying a transformation matrix.\"\"\"\n",
        "    if deficiency_type not in COLOR_BLINDNESS_MATRICES:\n",
        "        raise ValueError(f\"Invalid deficiency type. Choose from {list(COLOR_BLINDNESS_MATRICES.keys())}.\")\n",
        "\n",
        "    # If image is a file path, read it\n",
        "    if isinstance(image, str):\n",
        "        img = cv2.imread(image)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    else:\n",
        "        img = image.copy()\n",
        "        if len(img.shape) == 2:  # Grayscale\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "        elif img.shape[2] == 4:  # RGBA\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "    normalized_image = img.astype(float) / 255.0\n",
        "    transform_matrix = COLOR_BLINDNESS_MATRICES[deficiency_type]\n",
        "    transformed_image = np.dot(normalized_image.reshape(-1, 3), transform_matrix.T)\n",
        "    transformed_image = np.clip(transformed_image, 0, 1).reshape(img.shape)\n",
        "    transformed_image = (transformed_image * 255).astype(np.uint8)\n",
        "\n",
        "    return transformed_image\n",
        "\n",
        "def setup_detectron2_model():\n",
        "    \"\"\"Set up the Detectron2 configuration and predictor.\"\"\"\n",
        "    cfg = get_cfg()\n",
        "    cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "    return DefaultPredictor(cfg), cfg\n",
        "\n",
        "def detect_objects(image, predictor, cfg):\n",
        "    \"\"\"Detect objects using Detectron2 and return details.\"\"\"\n",
        "    # Convert to BGR for detectron2\n",
        "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    else:\n",
        "        image_bgr = image\n",
        "\n",
        "    outputs = predictor(image_bgr)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    boxes = instances.pred_boxes.tensor.numpy()\n",
        "    scores = instances.scores.numpy()\n",
        "    class_ids = instances.pred_classes.numpy()\n",
        "\n",
        "    # Get class names from metadata\n",
        "    metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "    class_names = metadata.thing_classes\n",
        "\n",
        "    # Visualize the detection output\n",
        "    v = Visualizer(image_bgr, metadata=metadata, scale=1.2)\n",
        "    vis_output = v.draw_instance_predictions(instances)\n",
        "    vis_image = vis_output.get_image()\n",
        "    vis_image = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for display\n",
        "\n",
        "    # Format output\n",
        "    detected_objects = []\n",
        "    masks = instances.pred_masks.numpy() if instances.has(\"pred_masks\") else None\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        class_name = class_names[class_ids[i]] if class_ids[i] < len(class_names) else \"Unknown\"\n",
        "        bbox_data = {\n",
        "            \"class\": class_name,\n",
        "            \"confidence\": float(scores[i]),\n",
        "            \"bounding_box\": {\n",
        "                \"x1\": float(boxes[i][0]),\n",
        "                \"y1\": float(boxes[i][1]),\n",
        "                \"x2\": float(boxes[i][2]),\n",
        "                \"y2\": float(boxes[i][3]),\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if masks is not None:\n",
        "            mask = (masks[i] * 255).astype(np.uint8)\n",
        "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            bbox_data[\"edges\"] = [contour.reshape(-1, 2).tolist() for contour in contours]\n",
        "\n",
        "        detected_objects.append(bbox_data)\n",
        "\n",
        "    return detected_objects, vis_image\n",
        "\n",
        "def save_detected_objects_with_edges(detected_objects, output_path):\n",
        "    \"\"\"Save detected objects with edges to a JSON file.\"\"\"\n",
        "    edge_data = [obj for obj in detected_objects if \"edges\" in obj]\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(edge_data, f, indent=2)\n",
        "    return output_path\n",
        "\n",
        "def analyze_iou_and_lab_similarity(image_rgb, json_path, color_threshold=20.0, iou_threshold=0):\n",
        "    \"\"\"Analyze detected objects based on IoU and LAB color similarity.\"\"\"\n",
        "    height, width = image_rgb.shape[:2]\n",
        "\n",
        "    with open(json_path, \"r\") as f:\n",
        "        objects = json.load(f)\n",
        "\n",
        "    # Convert contour edges to mask\n",
        "    def contour_to_mask(edges):\n",
        "        mask = np.zeros((height, width), dtype=np.uint8)\n",
        "        for edge in edges:\n",
        "            contour = np.array(edge, dtype=np.int32).reshape((-1, 1, 2))\n",
        "            cv2.drawContours(mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
        "        return mask\n",
        "\n",
        "    # Get average LAB color for the mask\n",
        "    def get_avg_lab(mask, image_rgb):\n",
        "        masked_pixels = image_rgb[mask == 255]\n",
        "        if masked_pixels.size == 0:\n",
        "            return np.zeros(3)\n",
        "        lab = rgb2lab(masked_pixels.reshape(-1, 1, 3) / 255.0).reshape(-1, 3)\n",
        "        return np.mean(lab, axis=0)\n",
        "\n",
        "    # Compute Intersection over Union (IoU)\n",
        "    def compute_iou(mask1, mask2):\n",
        "        inter = np.logical_and(mask1 == 255, mask2 == 255).sum()\n",
        "        union = np.logical_or(mask1 == 255, mask2 == 255).sum()\n",
        "        return inter / union if union > 0 else 0\n",
        "\n",
        "    # Prepare masks and LAB values for each object\n",
        "    masks = [contour_to_mask(obj[\"edges\"]) for obj in objects]\n",
        "    lab_means = [get_avg_lab(mask, image_rgb) for mask in masks]\n",
        "\n",
        "    # Compare all pairs based on IoU and LAB color similarity\n",
        "    similar_pairs = []\n",
        "    for i in range(len(objects)):\n",
        "        for j in range(i + 1, len(objects)):\n",
        "            iou = compute_iou(masks[i], masks[j])\n",
        "            if iou >= iou_threshold:\n",
        "                deltaE = np.linalg.norm(lab_means[i] - lab_means[j])\n",
        "                if deltaE < color_threshold:\n",
        "                    similar_pairs.append({\n",
        "                        \"obj1_index\": i,\n",
        "                        \"obj2_index\": j,\n",
        "                        \"label1\": objects[i][\"class\"],\n",
        "                        \"label2\": objects[j][\"class\"],\n",
        "                        \"iou\": float(iou),\n",
        "                        \"deltaE\": float(deltaE)\n",
        "                    })\n",
        "\n",
        "    # Highlight edges of similar objects\n",
        "    highlighted = image_rgb.copy()\n",
        "    for pair in similar_pairs:\n",
        "        for obj_id in [pair[\"obj1_index\"], pair[\"obj2_index\"]]:\n",
        "            contours = [np.array(edge, np.int32).reshape((-1, 1, 2)) for edge in objects[obj_id][\"edges\"]]\n",
        "            cv2.drawContours(highlighted, contours, -1, (0, 255, 0), thickness=3)\n",
        "\n",
        "    return similar_pairs, highlighted, masks\n",
        "\n",
        "def adjust_similar_object_colors(image_rgb, masks, similar_pairs, color_shift=60, shift_mode='auto'):\n",
        "    \"\"\"Shifts the color of one object in each similar-colored object pair.\"\"\"\n",
        "    # Convert to LAB for better color manipulation\n",
        "    lab_image = rgb2lab(image_rgb.astype(np.float32) / 255.0)\n",
        "    adjusted_image = lab_image.copy()\n",
        "    adjusted_ids = set()\n",
        "\n",
        "    for pair in similar_pairs:\n",
        "        target_id = pair[\"obj2_index\"]\n",
        "        if target_id in adjusted_ids:\n",
        "            continue  # Skip if already adjusted\n",
        "\n",
        "        mask = masks[target_id]\n",
        "        object_pixels = adjusted_image[mask == 255]\n",
        "\n",
        "        if object_pixels.size == 0:\n",
        "            continue  # Skip empty masks\n",
        "\n",
        "        # Compute mean LAB color\n",
        "        mean_color = np.mean(object_pixels, axis=0)\n",
        "\n",
        "        # Determine shift direction\n",
        "        if shift_mode == 'a':\n",
        "            shift = np.array([0, color_shift, 0])\n",
        "        elif shift_mode == 'b':\n",
        "            shift = np.array([0, 0, color_shift])\n",
        "        else:  # auto\n",
        "            shift = np.array([0, 0, 0])\n",
        "            if abs(mean_color[1]) < abs(mean_color[2]):\n",
        "                shift[1] = color_shift  # a channel\n",
        "            else:\n",
        "                shift[2] = color_shift  # b channel\n",
        "\n",
        "        # Apply shift\n",
        "        for i in range(3):\n",
        "            channel = adjusted_image[:, :, i]\n",
        "            channel[mask == 255] = np.clip(channel[mask == 255] + shift[i], -128 if i > 0 else 0, 127 if i > 0 else 100)\n",
        "            adjusted_image[:, :, i] = channel\n",
        "\n",
        "        adjusted_ids.add(target_id)\n",
        "\n",
        "    # Convert back to RGB\n",
        "    adjusted_rgb = lab2rgb(adjusted_image)\n",
        "    adjusted_rgb = (adjusted_rgb * 255).astype(np.uint8)\n",
        "\n",
        "    return adjusted_rgb\n",
        "\n",
        "def simulate_color_blindness_on_region(rgb_image, deficiency_type, region_mask):\n",
        "    \"\"\"Applies color blindness simulation only on a masked region of the image.\"\"\"\n",
        "    if deficiency_type not in COLOR_BLINDNESS_MATRICES:\n",
        "        raise ValueError(f\"Invalid deficiency type. Choose from {list(COLOR_BLINDNESS_MATRICES.keys())}.\")\n",
        "\n",
        "    transformed_image = rgb_image.copy().astype(np.float32) / 255.0\n",
        "    mask_indices = np.where(region_mask == 255)\n",
        "\n",
        "    # Extract only the masked pixels\n",
        "    pixels = transformed_image[mask_indices]\n",
        "    transformed_pixels = np.dot(pixels, COLOR_BLINDNESS_MATRICES[deficiency_type].T)\n",
        "    transformed_pixels = np.clip(transformed_pixels, 0, 1)\n",
        "\n",
        "    # Update only masked region in the image\n",
        "    transformed_image[mask_indices] = transformed_pixels\n",
        "    transformed_image = (transformed_image * 255).astype(np.uint8)\n",
        "\n",
        "    return transformed_image\n",
        "\n",
        "def get_default_shift_mode(deficiency_type):\n",
        "    \"\"\"Determine the optimal shift mode based on CVD type\"\"\"\n",
        "    if deficiency_type in ['protanopia', 'deuteranopia', 'protanomaly', 'deuteranomaly']:\n",
        "        # Red-green color blindness - shift on blue-yellow (b) axis\n",
        "        return 'b'\n",
        "    elif deficiency_type in ['tritanopia', 'tritanomaly']:\n",
        "        # Blue-yellow color blindness - shift on red-green (a) axis\n",
        "        return 'a'\n",
        "    else:\n",
        "        # For achromatopsia or achromatomaly, try both\n",
        "        return 'auto'\n",
        "\n",
        "def run_cvd_enhancement(image, deficiency_type, color_threshold, iou_threshold, color_shift, shift_mode, max_iterations):\n",
        "    \"\"\"Full pipeline for CVD enhancement\"\"\"\n",
        "    # Initialize outputs\n",
        "    results = {}\n",
        "\n",
        "    # Convert uploaded image to RGB\n",
        "    if image is None:\n",
        "        return None, None, None, None, None, \"Error: No image uploaded\"\n",
        "\n",
        "    if isinstance(image, str):\n",
        "        image = cv2.imread(image)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    elif len(image.shape) == 2:  # Grayscale\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    elif image.shape[2] == 4:  # RGBA\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "    # Save original image\n",
        "    original_path = \"/tmp/cvd_outputs/original.jpg\"\n",
        "    cv2.imwrite(original_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "    results[\"original\"] = image\n",
        "\n",
        "    # Step 1: Simulate color blindness\n",
        "    transformed_image = simulate_color_blindness(image, deficiency_type)\n",
        "    results[\"cvd_simulated\"] = transformed_image\n",
        "\n",
        "    # Step 2: Setup model and detect objects\n",
        "    predictor, cfg = setup_detectron2_model()\n",
        "    detected_objects, vis_image = detect_objects(image, predictor, cfg)\n",
        "    results[\"detection\"] = vis_image\n",
        "\n",
        "    # Step 3: Save detected objects\n",
        "    json_path = \"/tmp/cvd_outputs/detected_objects_with_edges.json\"\n",
        "    save_detected_objects_with_edges(detected_objects, json_path)\n",
        "\n",
        "    # Step 4: Analyze similarity\n",
        "    similar_pairs, highlighted, masks = analyze_iou_and_lab_similarity(\n",
        "        transformed_image,\n",
        "        json_path,\n",
        "        color_threshold=float(color_threshold),\n",
        "        iou_threshold=float(iou_threshold)\n",
        "    )\n",
        "    results[\"similar_objects\"] = highlighted\n",
        "\n",
        "    if not similar_pairs:\n",
        "        return (results.get(\"original\"),\n",
        "                results.get(\"cvd_simulated\"),\n",
        "                results.get(\"detection\"),\n",
        "                results.get(\"similar_objects\"),\n",
        "                results.get(\"enhanced_image\", transformed_image),\n",
        "                f\"No similar object pairs found. No enhancement needed.\")\n",
        "\n",
        "    # Step 5: Initialize for iterative enhancement\n",
        "    iteration = 0\n",
        "    adjusted_image = transformed_image.copy()\n",
        "\n",
        "    log_messages = [f\"Found {len(similar_pairs)} similar object pairs. Starting enhancement...\"]\n",
        "\n",
        "    # Step 6: Iterative enhancement\n",
        "    while iteration < int(max_iterations) and similar_pairs:\n",
        "        # Enhance colors\n",
        "        adjusted_rgb = adjust_similar_object_colors(\n",
        "            adjusted_image,\n",
        "            masks,\n",
        "            similar_pairs,\n",
        "            color_shift=float(color_shift),\n",
        "            shift_mode=shift_mode\n",
        "        )\n",
        "\n",
        "        # Create combined mask for all adjusted objects\n",
        "        adjusted_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "        for pair in similar_pairs:\n",
        "            adjusted_mask = cv2.bitwise_or(adjusted_mask, masks[pair[\"obj2_index\"]])\n",
        "\n",
        "        # Apply color blindness simulation to the adjusted regions\n",
        "        adjusted_image = simulate_color_blindness_on_region(adjusted_rgb, deficiency_type, adjusted_mask)\n",
        "\n",
        "        # Re-analyze similarity after adjustment\n",
        "        similar_pairs, highlighted, masks = analyze_iou_and_lab_similarity(\n",
        "            adjusted_image,\n",
        "            json_path,\n",
        "            color_threshold=float(color_threshold),\n",
        "            iou_threshold=float(iou_threshold)\n",
        "        )\n",
        "\n",
        "        log_messages.append(f\"Iteration {iteration + 1}: {len(similar_pairs)} similar pairs remaining\")\n",
        "\n",
        "        if len(similar_pairs) == 0:\n",
        "            log_messages.append(f\"All similar objects now distinguishable. Enhancement complete!\")\n",
        "            break\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    results[\"enhanced_image\"] = adjusted_image\n",
        "\n",
        "    # Return results\n",
        "    return (results.get(\"original\"),\n",
        "            results.get(\"cvd_simulated\"),\n",
        "            results.get(\"detection\"),\n",
        "            results.get(\"similar_objects\"),\n",
        "            results.get(\"enhanced_image\"),\n",
        "            \"\\n\".join(log_messages))\n",
        "\n",
        "# Create the Gradio interface\n",
        "def create_interface():\n",
        "    # Custom CSS for better styling\n",
        "    custom_css = \"\"\"\n",
        "    .app-header {\n",
        "        text-align: center;\n",
        "        margin: 0 auto;\n",
        "        max-width: 1200px;\n",
        "        padding: 1rem 0;\n",
        "    }\n",
        "\n",
        "    .app-title {\n",
        "        font-size: 2.5rem !important;\n",
        "        font-weight: bold;\n",
        "        margin-bottom: 0.5rem;\n",
        "        color: #1f77b4;\n",
        "    }\n",
        "\n",
        "    .app-description {\n",
        "        font-size: 1.2rem !important;\n",
        "        margin-bottom: 2rem;\n",
        "        max-width: 900px;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "    }\n",
        "\n",
        "    .how-it-works {\n",
        "        background-color: #222222;\n",
        "        color: #ffffff;\n",
        "        border-radius: 8px;\n",
        "        padding: 1.5rem;\n",
        "        margin-bottom: 2rem;\n",
        "        font-size: 1.2rem !important;\n",
        "        max-width: 1000px;\n",
        "        margin-left: auto;\n",
        "        margin-right: auto;\n",
        "        box-shadow: 0 2px 10px rgba(0,0,0,0.2);\n",
        "    }\n",
        "\n",
        "    .how-it-works h3 {\n",
        "        text-align: center;\n",
        "        font-size: 1.5rem !important;\n",
        "        margin-bottom: 1rem;\n",
        "        color: #ffffff;\n",
        "    }\n",
        "\n",
        "    .how-it-works ol {\n",
        "        padding-left: 2rem;\n",
        "    }\n",
        "\n",
        "    .how-it-works li {\n",
        "        margin-bottom: 0.5rem;\n",
        "    }\n",
        "\n",
        "    .how-it-works p {\n",
        "        margin-top: 1rem;\n",
        "    }\n",
        "\n",
        "    /* Fix image scaling */\n",
        "    .upload-image-container .wrap {\n",
        "        display: flex !important;\n",
        "        justify-content: center !important;\n",
        "        align-items: center !important;\n",
        "        height: auto !important;\n",
        "        min-height: 450px !important;\n",
        "    }\n",
        "\n",
        "    .upload-image-container img {\n",
        "        max-width: 100% !important;\n",
        "        max-height: 100% !important;\n",
        "        width: auto !important;\n",
        "        height: auto !important;\n",
        "        object-fit: contain !important;\n",
        "    }\n",
        "\n",
        "    .image-results-container {\n",
        "        flex-wrap: wrap !important;\n",
        "        justify-content: center !important;\n",
        "    }\n",
        "\n",
        "    /* Result images */\n",
        "    .result-image-container {\n",
        "        height: auto !important;\n",
        "        min-height: 450px !important;\n",
        "        display: flex !important;\n",
        "        align-items: center !important;\n",
        "        justify-content: center !important;\n",
        "    }\n",
        "\n",
        "    .result-image-container img {\n",
        "        max-width: 100% !important;\n",
        "        max-height: 100% !important;\n",
        "        width: auto !important;\n",
        "        height: auto !important;\n",
        "        object-fit: contain !important;\n",
        "        margin: 0 auto !important;\n",
        "    }\n",
        "\n",
        "    .results-header {\n",
        "        text-align: center;\n",
        "        font-size: 2rem !important;\n",
        "        margin: 1.5rem 0;\n",
        "    }\n",
        "\n",
        "    .result-column h3 {\n",
        "        text-align: center;\n",
        "        font-size: 1.5rem !important;\n",
        "        margin-bottom: 0.5rem;\n",
        "    }\n",
        "\n",
        "    .footer-button {\n",
        "        display: block;\n",
        "        margin: 2rem auto;\n",
        "        max-width: 300px;\n",
        "    }\n",
        "\n",
        "    /* Tab styling */\n",
        "    .detail-tabs .tab-nav {\n",
        "        background-color: #f5f5f5;\n",
        "        border-radius: 6px;\n",
        "        padding: 5px;\n",
        "    }\n",
        "\n",
        "    .detail-tabs .tabitem {\n",
        "        padding: 0.5rem !important;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"Color Vision Deficiency Enhancement Tool\", css=custom_css) as app:\n",
        "        # Title and description section (always visible)\n",
        "        with gr.Row(elem_classes=[\"app-header\"]):\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"# Color Vision Deficiency Enhancement Tool\", elem_classes=[\"app-title\"])\n",
        "                gr.Markdown(\n",
        "                    \"Transform images for better visibility by people with color vision deficiency. \"\n",
        "                    \"This intelligent system detects objects in images, identifies those that might appear similar to people with color blindness, \"\n",
        "                    \"and enhances color differences to make them more distinguishable.\",\n",
        "                    elem_classes=[\"app-description\"]\n",
        "                )\n",
        "\n",
        "        # How It Works section - centralized above the input section\n",
        "        with gr.Row(visible=True, elem_classes=[\"how-it-works-container\"]) as how_it_works_section:\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"\"\"\n",
        "                <div class=\"how-it-works\">\n",
        "                <h3>How It Works</h3>\n",
        "\n",
        "                <ol>\n",
        "                <li><strong>Upload</strong> your image that needs enhancement</li>\n",
        "                <li><strong>Select</strong> the type of color vision deficiency to accommodate</li>\n",
        "                <li>The system <strong>detects objects</strong> in your image using AI</li>\n",
        "                <li>It <strong>finds objects</strong> that might look similar to someone with the selected CVD</li>\n",
        "                <li>Colors are <strong>enhanced</strong> to make objects more distinguishable</li>\n",
        "                <li>View side-by-side <strong>comparisons</strong> of original, CVD simulation, and enhanced versions</li>\n",
        "                </ol>\n",
        "\n",
        "                <p>Fine-tune the enhancement with advanced settings for optimal results.</p>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "        # Input section\n",
        "        with gr.Row(visible=True) as input_section:\n",
        "            # Left panel - Image upload\n",
        "            with gr.Column(scale=3):\n",
        "                input_image = gr.Image(\n",
        "                    label=\"Upload Image\",\n",
        "                    type=\"numpy\",\n",
        "                    elem_classes=[\"upload-image-container\"],\n",
        "                    height=450,\n",
        "                    image_mode=\"RGB\",\n",
        "                    sources=[\"upload\", \"clipboard\"]\n",
        "                )\n",
        "\n",
        "            # Right panel - Settings\n",
        "            with gr.Column(scale=2):\n",
        "                deficiency_type = gr.Dropdown(\n",
        "                    choices=list(COLOR_BLINDNESS_MATRICES.keys()),\n",
        "                    value=\"protanopia\",\n",
        "                    label=\"Color Vision Deficiency Type\"\n",
        "                )\n",
        "\n",
        "                with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "                    color_threshold = gr.Slider(minimum=5, maximum=50, value=20, step=1,\n",
        "                                              label=\"Color Similarity Threshold (Î”E)\")\n",
        "                    iou_threshold = gr.Slider(minimum=0, maximum=0.5, value=0, step=0.01,\n",
        "                                            label=\"Object Overlap Threshold (IoU)\")\n",
        "                    color_shift = gr.Slider(minimum=20, maximum=100, value=50, step=5,\n",
        "                                          label=\"Color Shift Amount\")\n",
        "                    shift_mode = gr.Radio([\"auto\", \"a\", \"b\"], value=\"b\",\n",
        "                                        label=\"Color Shift Mode\")\n",
        "                    max_iterations = gr.Slider(minimum=1, maximum=10, value=3, step=1,\n",
        "                                             label=\"Maximum Enhancement Iterations\")\n",
        "\n",
        "                # Update shift mode based on deficiency type\n",
        "                deficiency_type.change(\n",
        "                    fn=lambda x: gr.update(value=get_default_shift_mode(x)),\n",
        "                    inputs=[deficiency_type],\n",
        "                    outputs=[shift_mode]\n",
        "                )\n",
        "\n",
        "                run_button = gr.Button(\"Run Enhancement\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        # Processing log (temporarily visible during processing)\n",
        "        with gr.Row(visible=False) as processing_section:\n",
        "            output_log = gr.Textbox(label=\"Processing Status\", lines=3)\n",
        "\n",
        "        # Results header\n",
        "        with gr.Row(visible=False, elem_classes=[\"results-header\"]) as results_header:\n",
        "            gr.Markdown(\"## Results\")\n",
        "\n",
        "        # Main results row - showing 3 main images\n",
        "        with gr.Row(visible=False, equal_height=True) as main_results:\n",
        "            with gr.Column(elem_classes=[\"result-column\"]):\n",
        "                gr.Markdown(\"### Original\")\n",
        "                original_output = gr.Image(elem_classes=[\"result-image-container\"])\n",
        "\n",
        "            with gr.Column(elem_classes=[\"result-column\"]):\n",
        "                gr.Markdown(\"### CVD Simulation\")\n",
        "                cvd_output = gr.Image(elem_classes=[\"result-image-container\"])\n",
        "\n",
        "            with gr.Column(elem_classes=[\"result-column\"]):\n",
        "                gr.Markdown(\"### Enhanced for CVD\")\n",
        "                enhanced_output = gr.Image(elem_classes=[\"result-image-container\"])\n",
        "\n",
        "        # Optional detailed view with tabs (same scale as main results)\n",
        "        with gr.Row(visible=False) as detailed_results:\n",
        "            # Use a Column to make it span the full width\n",
        "            with gr.Column():\n",
        "                with gr.Tabs(elem_classes=[\"detail-tabs\"]) as tabs:\n",
        "                    # FIX: Remove the 'selected=True' parameter from TabItem\n",
        "                    with gr.TabItem(\"Object Detection\", id=\"tab-detection\"):\n",
        "                        detection_output = gr.Image(\n",
        "                            elem_classes=[\"result-image-container\"],\n",
        "                            height=350\n",
        "                        )\n",
        "\n",
        "                    with gr.TabItem(\"Similar Objects\", id=\"tab-similar\"):\n",
        "                        similar_output = gr.Image(\n",
        "                            elem_classes=[\"result-image-container\"],\n",
        "                            height=350\n",
        "                        )\n",
        "\n",
        "        # Rerun button (visible after processing)\n",
        "        with gr.Row(visible=False) as rerun_section:\n",
        "            rerun_button = gr.Button(\"Process Another Image\", variant=\"secondary\", size=\"lg\", elem_classes=[\"footer-button\"])\n",
        "\n",
        "        # Define workflow\n",
        "        def show_processing():\n",
        "            return {\n",
        "                input_section: gr.update(visible=False),\n",
        "                how_it_works_section: gr.update(visible=False),\n",
        "                processing_section: gr.update(visible=True),\n",
        "                results_header: gr.update(visible=False),\n",
        "                main_results: gr.update(visible=False),\n",
        "                detailed_results: gr.update(visible=False),\n",
        "                rerun_section: gr.update(visible=False)\n",
        "            }\n",
        "\n",
        "        def show_results():\n",
        "            return {\n",
        "                input_section: gr.update(visible=False),\n",
        "                how_it_works_section: gr.update(visible=False),\n",
        "                processing_section: gr.update(visible=False),\n",
        "                results_header: gr.update(visible=True),\n",
        "                main_results: gr.update(visible=True),\n",
        "                detailed_results: gr.update(visible=True),\n",
        "                rerun_section: gr.update(visible=True)\n",
        "            }\n",
        "\n",
        "        def reset_interface():\n",
        "            return {\n",
        "                input_section: gr.update(visible=True),\n",
        "                how_it_works_section: gr.update(visible=True),\n",
        "                processing_section: gr.update(visible=False),\n",
        "                results_header: gr.update(visible=False),\n",
        "                main_results: gr.update(visible=False),\n",
        "                detailed_results: gr.update(visible=False),\n",
        "                rerun_section: gr.update(visible=False)\n",
        "            }\n",
        "\n",
        "        # Connect event handlers\n",
        "        run_button.click(\n",
        "            fn=show_processing,\n",
        "            outputs=[input_section, how_it_works_section, processing_section, results_header, main_results, detailed_results, rerun_section]\n",
        "        ).then(\n",
        "            fn=run_cvd_enhancement,\n",
        "            inputs=[input_image, deficiency_type, color_threshold, iou_threshold, color_shift, shift_mode, max_iterations],\n",
        "            outputs=[original_output, cvd_output, detection_output, similar_output, enhanced_output, output_log]\n",
        "        ).then(\n",
        "            fn=show_results,\n",
        "            outputs=[input_section, how_it_works_section, processing_section, results_header, main_results, detailed_results, rerun_section]\n",
        "        )\n",
        "\n",
        "        rerun_button.click(\n",
        "            fn=reset_interface,\n",
        "            outputs=[input_section, how_it_works_section, processing_section, results_header, main_results, detailed_results, rerun_section]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Load and run the interface\n",
        "demo = create_interface()\n",
        "demo.launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
